import { QuizResponse, AnalysisResult } from './types';
import { SemanticResults, SemanticCandidate } from './semantic-search';
import { generateText } from 'ai';

interface LLMSelectionResult {
  selectedStudent?: string;
  selectedFaculty?: string;
  selectedAlumni?: string | null;
  reasoning: string;
  matchScore: number;
  personalizedMessage: string;
  confidence?: number;
  processingTimeMs: number;
}

interface LLMSelectionEngine {
  selectOptimalMatches(
    quiz: QuizResponse,
    semanticResults: SemanticResults
  ): Promise<LLMSelectionResult>;
}

class GrokSelectionEngine implements LLMSelectionEngine {
  private aiGatewayKey: string;
  private openRouterKey: string;
  private baseURL: string;
  private model: string;
  private useVercelGateway: boolean;

  constructor() {
    this.aiGatewayKey = process.env.AI_GATEWAY_API_KEY || '';
    this.openRouterKey = process.env.OPENROUTER_API_KEY || '';
    this.baseURL = process.env.OPENROUTER_BASE_URL || 'https://openrouter.ai/api/v1';
    this.model = process.env.CHAT_MODEL || process.env.OPENROUTER_MODEL || 'xai/grok-4-fast-non-reasoning';

    // Use Vercel AI Gateway if available, otherwise fallback to OpenRouter
    this.useVercelGateway = !!(this.aiGatewayKey && this.aiGatewayKey !== 'your_ai_gateway_api_key_here');

    if (!this.useVercelGateway && !this.openRouterKey) {
      throw new Error('Either AI_GATEWAY_API_KEY or OPENROUTER_API_KEY is required for LLM selection');
    }

    if (this.useVercelGateway) {
      console.log('üöÄ Using Vercel AI Gateway for LLM selection (Grok ‚Üí GPT-4o-mini ‚Üí OpenRouter fallback chain)');
    } else {
      console.log('üîÑ Using OpenRouter fallback for LLM selection');
    }
  }

  /**
   * Build rich system prompt for admissions director role
   */
  private buildSystemPrompt(): string {
    return [
      'You are the experienced admissions director at Saint Stephen\'s Episcopal School with 20+ years of helping families find perfect educational fits.',
      '',
      'Your expertise includes:',
      '- Understanding child development and learning styles',
      '- Matching student personalities with faculty mentorship styles',
      '- Creating compelling admissions narratives that drive enrollment',
      '- Balancing authentic fit with strategic enrollment goals',
      '',
      'SELECTION CRITERIA (in order of importance):',
      '1. PERSONALITY FIT: How well does the child\'s description match the student story and faculty approach?',
      '2. DEVELOPMENTAL STAGE: Are the stories/faculty appropriate for the child\'s grade and maturity?',
      '3. NARRATIVE ARC: Do the selections create a compelling story of growth from current student ‚Üí faculty mentorship ‚Üí alumni outcomes?',
      '4. ENROLLMENT APPEAL: Will these selections convince this specific family to choose Saint Stephen\'s?',
      '',
      'CONSTRAINTS:',
      '- Must select exactly 1 current student (required)',
      '- Must select exactly 1 faculty member (required)',
      '- May select 1 alumni if there\'s a compelling match (optional)',
      '- STRONGLY PRIORITIZE VIDEO CONTENT: When fit quality is equal, always choose candidates with "Video: Available ‚úì"',
      '- Video testimonials create powerful emotional connections that drive enrollment decisions',
      '- Current student videos are most compelling - they show authentic peer experiences',
      '- Faculty videos demonstrate teaching style and personality beyond text descriptions',
      '- Consider family timeline and urgency',
      '',
      'OUTPUT FORMAT:',
      'Return valid JSON only, no additional text:',
      '{',
      '  "selectedStudent": "story_id",',
      '  "selectedFaculty": "faculty_id",',
      '  "selectedAlumni": "alumni_id_or_null",',
      '  "reasoning": "2-3 sentences explaining why these selections create the perfect admissions story for this specific child and family",',
      '  "matchScore": 85-100,',
      '  "personalizedMessage": "Warm, specific 2-4 sentence message for the visiting family that connects their child to these selections and invites engagement"',
      '}'
    ].join('\n');
  }

  /**
   * Build detailed user prompt with candidate information
   */
  private buildUserPrompt(quiz: QuizResponse, semanticResults: SemanticResults): string {
    const formatCandidate = (candidate: SemanticCandidate, index: number) => {
      const metadata = candidate.metadata;

      if (candidate.category === 'student') {
        return [
          `${index + 1}. ID: ${candidate.id}`,
          `   Name: ${metadata.firstName}`,
          `   Story: ${metadata.achievement}`,
          `   Interests: ${metadata.interests?.join(', ') || 'N/A'}`,
          `   Personality: ${metadata.personaDescriptors?.join(', ') || 'N/A'}`,
          `   Grade Focus: ${metadata.gradeBands?.join('/') || metadata.gradeLevel || 'All'}`,
          `   Video: ${candidate.hasVideo ? 'Available ‚úì' : 'None'}`,
          `   Similarity: ${candidate.semanticScore}`,
          `   Parent Quote: "${metadata.parentQuote || 'N/A'}"`,
          `   Student Quote: "${metadata.studentQuote || 'N/A'}"`
        ].join('\n');
      } else if (candidate.category === 'faculty') {
        return [
          `${index + 1}. ID: ${candidate.id}`,
          `   Name: ${metadata.formalTitle || 'Mr./Ms.'} ${metadata.lastName || metadata.firstName}`,
          `   Title: ${metadata.title}`,
          `   Department: ${metadata.department || 'N/A'}`,
          `   Specializes: ${metadata.specializesIn?.join(', ') || 'N/A'}`,
          `   Why Students Love: ${metadata.whyStudentsLoveThem}`,
          `   Grade Focus: ${metadata.gradeBands?.join('/') || 'All Levels'}`,
          `   Experience: ${metadata.yearsAtSSES ? metadata.yearsAtSSES + ' years' : 'N/A'}`,
          `   Video: ${candidate.hasVideo ? 'Available ‚úì' : 'None'}`,
          `   Similarity: ${candidate.semanticScore}`,
          `   Keywords: ${metadata.interestKeywords?.join(', ') || 'N/A'}`
        ].join('\n');
      } else { // alumni
        return [
          `${index + 1}. ID: ${candidate.id}`,
          `   Name: ${metadata.firstName} ${metadata.lastName || ''}`,
          `   Class: ${metadata.classYear || 'N/A'}`,
          `   Current Role: ${metadata.currentRole || 'N/A'}`,
          `   Achievement: ${metadata.achievement}`,
          `   Interests: ${metadata.interests?.join(', ') || 'N/A'}`,
          `   Video: ${candidate.hasVideo ? 'Available ‚úì' : 'None'}`,
          `   Similarity: ${candidate.semanticScore}`,
          `   Quote: "${metadata.quote || 'N/A'}"`
        ].join('\n');
      }
    };

    return [
      'FAMILY PROFILE:',
      `Child Description: "${quiz.threeWords || quiz.childDescription || 'Not provided'}" (PRIMARY MATCHING FACTOR)`,
      `Selected Interests: ${(quiz.interests || []).join(', ') || 'None specified'}`,
      `Grade Level: ${quiz.gradeLevel}`,
      `Family Values: ${(quiz.familyValues || []).join(', ') || 'None specified'}`,
      `Timeline: ${quiz.timeline}`,
      `Additional Context: ${quiz.additionalNotes || 'None'}`,
      '',
      'SEMANTIC SEARCH RESULTS:',
      `Query Processing: Found ${semanticResults.students.length + semanticResults.faculty.length + semanticResults.alumni.length} relevant candidates in ${semanticResults.processingTimeMs}ms`,
      '',
      'CURRENT STUDENT OPTIONS (choose exactly 1):',
      semanticResults.students.length > 0
        ? semanticResults.students.map(formatCandidate).join('\n\n')
        : 'No current student candidates found',
      '',
      'FACULTY OPTIONS (choose exactly 1):',
      semanticResults.faculty.length > 0
        ? semanticResults.faculty.map(formatCandidate).join('\n\n')
        : 'No faculty candidates found',
      '',
      'ALUMNI OPTIONS (choose 1 if compelling match, or none):',
      semanticResults.alumni.length > 0
        ? semanticResults.alumni.map(formatCandidate).join('\n\n')
        : 'No alumni candidates found',
      '',
      'TASK:',
      'As Saint Stephen\'s admissions director, analyze this family profile and select the optimal matches that will:',
      '1. Best match the child\'s personality and interests',
      '2. Create the most compelling admissions narrative',
      '3. Maximize enrollment likelihood for this specific family',
      '4. PRIORITIZE VIDEO CONTENT: Choose video-enabled candidates when personality/interest fit is comparable',
      '',
      'DECISION FRAMEWORK:',
      '‚Ä¢ If multiple candidates fit equally well ‚Üí CHOOSE THE ONE WITH VIDEO',
      '‚Ä¢ Video testimonials show authentic student/faculty experiences',
      '‚Ä¢ Parents connecting with video stories leads to campus visits and enrollment',
      '‚Ä¢ Focus heavily on the child description - this is the most important factor',
      '',
      'Return your selections in the specified JSON format with reasoning that mentions video prioritization when applicable.'
    ].join('\n');
  }

  /**
   * Call LLM API for selection using Vercel AI SDK (with automatic OpenAI and OpenRouter fallbacks)
   */
  private async callLLMAPI(systemPrompt: string, userPrompt: string): Promise<any> {
    // Try Vercel AI Gateway first if configured (using AI SDK)
    if (this.useVercelGateway) {
      try {
        const result = await generateText({
          model: this.model,
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          temperature: 0.3,
          maxOutputTokens: 800,
        });

        if (!result.text) {
          throw new Error('No content received from Vercel AI Gateway');
        }

        return JSON.parse(result.text);
      } catch (error) {
        console.warn('‚ö†Ô∏è Vercel AI Gateway (Grok) failed, trying GPT-4o-mini via AI Gateway:', error);

        // Try GPT-4o-mini via AI Gateway as fallback
        try {
          return await this.callAIGatewayFallback(systemPrompt, userPrompt);
        } catch (gptError) {
          console.warn('‚ö†Ô∏è GPT-4o-mini via AI Gateway failed, falling back to OpenRouter:', gptError);
        }

        // Final fallback to OpenRouter
        if (!this.openRouterKey) {
          throw new Error('All LLM providers failed and no additional fallbacks available');
        }
      }
    }

    // Use OpenRouter as final fallback
    return await this.callOpenRouterAPI(systemPrompt, userPrompt);
  }

  /**
   * Call GPT-4o-mini via AI Gateway (fallback)
   */
  private async callAIGatewayFallback(systemPrompt: string, userPrompt: string): Promise<any> {
    console.log('üß† Using GPT-4o-mini via AI Gateway fallback...');

    const result = await generateText({
      model: 'openai/gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.3,
      maxOutputTokens: 800,
    });

    if (!result.text) {
      throw new Error('No content received from GPT-4o-mini via AI Gateway');
    }

    return JSON.parse(result.text);
  }

  /**
   * Call OpenRouter API (final fallback using raw fetch)
   */
  private async callOpenRouterAPI(systemPrompt: string, userPrompt: string): Promise<any> {
    // Use OpenRouter model format for direct calls
    const openRouterModel = this.model.startsWith('xai/')
      ? this.model.replace('xai/', 'x-ai/') + ':free'
      : process.env.OPENROUTER_MODEL || 'x-ai/grok-4-fast:free';

    console.log('üîÑ Using OpenRouter fallback with model:', openRouterModel);

    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openRouterKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': process.env.NEXT_PUBLIC_SITE_URL || 'http://localhost:3000',
      },
      body: JSON.stringify({
        model: openRouterModel,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature: 0.3,
        max_tokens: 800,
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`OpenRouter API error: ${response.status} ${response.statusText} - ${errorText}`);
    }

    const data = await response.json();
    const content = data?.choices?.[0]?.message?.content;

    if (!content) {
      throw new Error('No content received from OpenRouter API');
    }

    return JSON.parse(content);
  }

  /**
   * Main selection method
   */
  async selectOptimalMatches(
    quiz: QuizResponse,
    semanticResults: SemanticResults
  ): Promise<LLMSelectionResult> {
    const startTime = Date.now();

    try {
      const systemPrompt = this.buildSystemPrompt();
      const userPrompt = this.buildUserPrompt(quiz, semanticResults);

      console.log('üß† Calling LLM for AI-powered selection...');
      const llmResponse = await this.callLLMAPI(systemPrompt, userPrompt);

      // Validate LLM response
      if (!llmResponse.selectedStudent || !llmResponse.selectedFaculty) {
        throw new Error('LLM did not select required student and faculty');
      }

      // Verify selections exist in semantic results
      const selectedStudentExists = semanticResults.students.some(s => s.id === llmResponse.selectedStudent);
      const selectedFacultyExists = semanticResults.faculty.some(f => f.id === llmResponse.selectedFaculty);

      if (!selectedStudentExists) {
        throw new Error(`LLM selected invalid student ID: ${llmResponse.selectedStudent}`);
      }
      if (!selectedFacultyExists) {
        throw new Error(`LLM selected invalid faculty ID: ${llmResponse.selectedFaculty}`);
      }

      const processingTimeMs = Date.now() - startTime;

      return {
        selectedStudent: llmResponse.selectedStudent,
        selectedFaculty: llmResponse.selectedFaculty,
        selectedAlumni: llmResponse.selectedAlumni,
        reasoning: llmResponse.reasoning,
        matchScore: Math.max(85, Math.min(100, llmResponse.matchScore || 90)),
        personalizedMessage: llmResponse.personalizedMessage,
        processingTimeMs
      };

    } catch (error) {
      console.error('‚ùå LLM selection failed:', error);
      throw error;
    }
  }
}

/**
 * Factory function to create appropriate LLM engine
 */
export function createLLMSelectionEngine(): LLMSelectionEngine {
  return new GrokSelectionEngine();
}

export { GrokSelectionEngine, type LLMSelectionResult, type LLMSelectionEngine };